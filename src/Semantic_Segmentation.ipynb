{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgtxk2vhTDsc"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 The Earth Engine Community Authors { display-mode: \"form\" }\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFPmkPvl1Rvb"
   },
   "source": [
    "# Symantic Segmentation of High Resolution Imagery\n",
    "Author: Mort Canty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0rGG5Zq-U_T"
   },
   "source": [
    "## Context\n",
    " \n",
    "[Fully convolutional neural networks](https://ieeexplore.ieee.org/document/7478072) (FCNs) are commonly used for [semantic image classification or segmentation](https://nanonets.com/blog/semantic-image-segmentation-2020/), essentially the assignment of every pixel in an image to one of two or more categories. In this tutorial we examine a popular FCN architecture, called [UNet](https://arxiv.org/abs/1505.04597) to perform a specific semantic segmentation task, namely urban building recognition: the identification within an arbitrarily complex remote sensing image of houses, schools, commercial edifices, etc. Here for example is a recent application of the UNet architecture for large scale [semantic classification of building footprints](https://arxiv.org/abs/2107.12283) over the entire African continent.  \n",
    "\n",
    "To train our model we will make use of the dataset  for the [INRIA Aerial Imaging Labeling Benchmark](https://hal.inria.fr/hal-01468452/document), published in 2016. The dataset, consisting of 30cm spatial resolution images and semantic labels over built up regions in Europe and USA, can be downloaded [here](https://project.inria.fr/aerialimagelabeling/). A presentation of the most successful semantic classifiers determined in the early part of the benchmark competition is given [here](https://hal.inria.fr/hal-01767807/document). \n",
    "\n",
    "After programming and training a UNet network at a **fourfold reduced spatial resolution** to that used in the competition, we'll apply it to images outside the INRIA training/test domain by segmenting aerial and satellite imagery from the [GEE high resolution image archive](https://developers.google.com/earth-engine/datasets/tags/highres)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Tr3BY_mhv-1"
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVotLC6T7uCf"
   },
   "outputs": [],
   "source": [
    "# Setup the GEE Python API\n",
    "import ee\n",
    "# Trigger the authentication flow.\n",
    "ee.Authenticate()\n",
    "# Initialize the library.\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "M730JvEN1Rvm"
   },
   "outputs": [],
   "source": [
    "# Import python modules\n",
    "import os, sys\n",
    "from osgeo import gdal,gdalconst\n",
    "from osgeo.gdalconst import GA_ReadOnly\n",
    "import numpy as np\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBJjHr_AEXbQ"
   },
   "outputs": [],
   "source": [
    "# Import the Folium library.\n",
    "import folium\n",
    "from folium.plugins import Fullscreen # won't work on Colab\n",
    "\n",
    "# Define a method for displaying Earth Engine image tiles to folium map.\n",
    "def add_ee_layer(self, ee_image_object, vis_params, name):\n",
    "    map_id_dict = ee.Image(ee_image_object).getMapId(vis_params)\n",
    "    folium.raster_layers.TileLayer(\n",
    "        tiles = map_id_dict['tile_fetcher'].url_format,\n",
    "        attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
    "        name = name,\n",
    "        overlay = True,\n",
    "        control = True\n",
    "    ).add_to(self)\n",
    "# Add EE drawing method to folium.\n",
    "folium.Map.add_ee_layer = add_ee_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ojXn_BJ5Xnh"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive \n",
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjC6OEZkv585"
   },
   "outputs": [],
   "source": [
    "# query GPU device\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTGksZYmN2_c"
   },
   "source": [
    "## The training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e34hnigoL14l"
   },
   "source": [
    "To quote from the INRIA download site:\n",
    "\n",
    " _[the images] cover dissimilar urban settlements, ranging from densely populated areas (e.g., San Franciscoâ€™s financial district) to alpine towns (e.g,. Lienz in Austrian Tyrol)-_\n",
    "\n",
    "There are 180 image/label pairs in all. Here is is an example of a full $5000\\times 5000$-pixel image/label pair over Chicago after upload to GEE and projection onto a folium map. For comparison and for later use we also display the corresponding spatial subset of the GEE's [USDA NAIP dataset](https://developers.google.com/earth-engine/datasets/catalog/USDA_NAIP_DOQQ). Note the poorer spatial resolution of the NAIP image (circa 1m) relative to the INRIA image (30cm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dofmxlULF-0K"
   },
   "outputs": [],
   "source": [
    "img = ee.Image('users/mortcanty/chicago3')\n",
    "label = ee.Image('users/mortcanty/chicago_label3')\n",
    "img_rgb = ee.Image.rgb(img.select(0),img.select(1),img.select(2))\n",
    "\n",
    "location = img.geometry().centroid().coordinates().getInfo()[::-1]\n",
    "m = folium.Map(location=location, zoom_start=15, height=800, width=1000)\n",
    "\n",
    "m.add_ee_layer(img_rgb, {'min': 0, 'max': 200}, 'INRIA Train Image')\n",
    "m.add_ee_layer(label, {'min': 0, 'max': 200}, 'INRIA Label Image')\n",
    "\n",
    "naip = ee.ImageCollection('USDA/NAIP/DOQQ') \\\n",
    "                      .select(['R','G','B']) \\\n",
    "                      .filter(ee.Filter.date('2015-01-01', '2015-12-31')) \\\n",
    "                      .filterBounds(img.geometry()) \\\n",
    "                      .mosaic() \\\n",
    "                      .clip(img.geometry()) \n",
    "\n",
    "naip_rgb = ee.Image.rgb(naip.select(0),naip.select(1),naip.select(2))\n",
    "m.add_ee_layer(naip, {'min': 0, 'max': 255}, 'NAIP Image')\n",
    "\n",
    "m.add_child(folium.LayerControl())\n",
    "\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HyjiIy81Rvn"
   },
   "source": [
    "### Preprocessing\n",
    "The following code preprocesses the INRIA image/label pairs and is given here mainly for completeness. The preprocessed and compressed train/test data are in any case shared as the files _images_trainx.npz_ and _images_testx.npz_. If you wish to repeat or modify the preprocessing, the cells below should be run on your local machine, especially if disk space is a problem on Google Drive. The INRIA training data occupy 13 GB before compression but only about 1.5 GB when compressed and augmented (see below). \n",
    "\n",
    "We assume here that the train images are in the directory _train_folder/images_ and the labels in the diretory _train_folder/gt_. The preprocessor (somewhat wastefully) clips the original $5000\\times 5000$ pixel images and labels to $4096\\times 4096$ and then resamples the result to $1024\\times 1024$, thus generating training and test image sets with approx 1.2m ground resolution. Each image (and its label) is then split into four $512\\times 512$ segments.  There are 720 image/label pairs, which are then augmented by  (1) by applying a linear 2% saturation stretch, (2) a histogram equalization and (3) horizontal flips. This is a so-called *regularization* measure which, it is hoped, will prevent overfitting and improve generalization. The image and label segments are shuffled randomly and the resulting numpy arrays are saved in [compressed .npz format](https://numpy.org/doc/stable/reference/generated/numpy.savez_compressed.html) to the train_folder directory. This results in 2880 images altogether, of which 400 are split off for testing and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7MyNU0ri1Rvo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images array:\n",
      "0  80  160  240  320  400  480  560  640  720  800  880  960  1040  1120  1200  1280  1360  1440  1520  1600  1680  1760  1840  1920  2000  2080  2160  2240  2320  2400  2480  2560  2640  2720  2800  \n",
      "labels array:\n",
      "0  80  160  240  320  400  480  560  640  720  800  880  960  1040  1120  1200  1280  1360  1440  1520  1600  1680  1760  1840  1920  2000  2080  2160  2240  2320  2400  2480  2560  2640  2720  2800  "
     ]
    }
   ],
   "source": [
    "#@title Preprocessor\n",
    "import os, sys\n",
    "from osgeo import gdal,gdalconst\n",
    "from osgeo.gdalconst import GA_ReadOnly\n",
    "\n",
    "# insert your (local) path here\n",
    "train_folder = '/media/mort/Crucial/imagery/Inria/AerialImageDataset/train'\n",
    "\n",
    "def bytestretch(arr,rng=None):\n",
    "#   byte stretch image numpy array\n",
    "    shp = arr.shape\n",
    "    arr = arr.ravel()\n",
    "    if rng is None:\n",
    "        rng = [np.min(arr),np.max(arr)]\n",
    "    tmp =  (arr-rng[0])*255.0/(rng[1]-rng[0])\n",
    "    tmp = np.where(tmp<0,0,tmp)  \n",
    "    tmp = np.where(tmp>255,255,tmp) \n",
    "    return np.asarray(np.reshape(tmp,shp),np.uint8)\n",
    "\n",
    "def histeqstr(x):\n",
    "#   histogram equalization stretch        \n",
    "    x = bytestretch(x)\n",
    "    hist,bin_edges = np.histogram(x,256,(0,256))\n",
    "    cdf = hist.cumsum()\n",
    "    lut = 255*cdf/float(cdf[-1])\n",
    "    return np.interp(x,bin_edges[:-1],lut)\n",
    "\n",
    "def lin2pcstr(x):\n",
    "#  2% linear stretch\n",
    "    x = bytestretch(x)\n",
    "    hist,bin_edges = np.histogram(x,256,(0,256))\n",
    "    cdf = hist.cumsum()\n",
    "    lower = 0\n",
    "    i = 0\n",
    "    while cdf[i] < 0.02*cdf[-1]:\n",
    "        lower += 1\n",
    "        i += 1\n",
    "    upper = 255\n",
    "    i = 255\n",
    "    while (cdf[i] > 0.98*cdf[-1]) and (upper>100):\n",
    "        upper -= 1\n",
    "        i -= 1\n",
    "    fp = (bin_edges-lower)*255/(upper-lower)\n",
    "    fp = np.where(bin_edges<=lower,0,fp)\n",
    "    fp = np.where(bin_edges>=upper,255,fp)\n",
    "    return np.interp(x,bin_edges,fp)          \n",
    "\n",
    "def make_traintest_arrays(folder,tmp='tmp.jpg'):\n",
    "\n",
    "    gdal.AllRegister()\n",
    "    translate_options = gdal.TranslateOptions(format='JPEG',outputType=gdalconst.GDT_Byte,\n",
    "                                              resampleAlg='bilinear',\n",
    "                                              width=1024,height=1024,srcWin=[0,0,4096,4096])\n",
    "#  images\n",
    "    files = os.listdir(folder+'/images')\n",
    "    files.sort()\n",
    "    num_files = len(files)\n",
    "    idx = np.random.permutation(range(16*num_files)) \n",
    "    images = np.zeros((16*num_files,512,512,3),dtype=np.uint8)\n",
    "    i = 0\n",
    "    print('images array:')\n",
    "    for file in files: \n",
    "        if i%10 == 0:\n",
    "            print( '%i '%i,end=' ') \n",
    "        gdal.Translate(tmp,folder+'/images/'+file,options=translate_options)\n",
    "        img = np.reshape(np.asarray(Image.open(tmp)),(1024,1024,3))\n",
    "        images[i,:,:,:] =   img[:512,:512,:]\n",
    "        images[i+1,:,:,:] = img[:512,512:,:]\n",
    "        images[i+2,:,:,:] = img[512:,:512,:]\n",
    "        images[i+3,:,:,:] = img[512:,512:,:]   \n",
    "#      augmentation linear saturation stretch \n",
    "        img2pc = img*0\n",
    "        for k in range(3):\n",
    "            img2pc[:,:,k] = lin2pcstr(img[:,:,k])\n",
    "        images[i+4,:,:,:] = img2pc[:512,:512,:]\n",
    "        images[i+5,:,:,:] = img2pc[:512,512:,:]\n",
    "        images[i+6,:,:,:] = img2pc[512:,:512,:]\n",
    "        images[i+7,:,:,:] = img2pc[512:,512:,:]\n",
    "#      augmentation histogram equalization stretch        \n",
    "        imgheq = img*0\n",
    "        for k in range(3):\n",
    "            imgheq[:,:,k] = histeqstr(img[:,:,k])\n",
    "        images[i+8,:,:,:] = imgheq[:512,:512,:]\n",
    "        images[i+9,:,:,:] = imgheq[:512,512:,:]\n",
    "        images[i+10,:,:,:] = imgheq[512:,:512,:]\n",
    "        images[i+11,:,:,:] = imgheq[512:,512:,:]        \n",
    "#      augmentation (left to right flip)         \n",
    "        images[i+12,:,:,:] = np.flip(img[:512,:512,:],1)\n",
    "        images[i+13,:,:,:] = np.flip(img[:512,512:,:],1)\n",
    "        images[i+14,:,:,:] = np.flip(img[512:,:512,:],1)\n",
    "        images[i+15,:,:,:] = np.flip(img[512:,512:,:],1)        \n",
    "        i += 16\n",
    "#  shuffle        \n",
    "    images = images[idx,:,:,:]    \n",
    "#  labels    \n",
    "    files = os.listdir(folder+'/gt')\n",
    "    files.sort()\n",
    "    labels = np.zeros((16*num_files,512,512,1),dtype=np.float32)\n",
    "    i = 0\n",
    "    print('\\nlabels array:')\n",
    "    for file in files:\n",
    "        if i%10 == 0:\n",
    "            print( '%i '%i,end=' ') \n",
    "        gdal.Translate(tmp,folder+'/gt/'+file,options=translate_options)\n",
    "        img = np.reshape(np.asarray(Image.open(tmp)),(1024,1024,1))\n",
    "        img = np.where(img>200,1,0)\n",
    "        labels[i,:,:,:] =   img[:512,:512,:]\n",
    "        labels[i+1,:,:,:] = img[:512,512:,:]\n",
    "        labels[i+2,:,:,:] = img[512:,:512,:]\n",
    "        labels[i+3,:,:,:] = img[512:,512:,:]\n",
    "#      augmentation        \n",
    "        labels[i+4,:,:,:] = img[:512,:512,:]\n",
    "        labels[i+5,:,:,:] = img[:512,512:,:]\n",
    "        labels[i+6,:,:,:] = img[512:,:512,:]\n",
    "        labels[i+7,:,:,:] = img[512:,512:,:]\n",
    "#      augmentation        \n",
    "        labels[i+8,:,:,:] = img[:512,:512,:]\n",
    "        labels[i+9,:,:,:] = img[:512,512:,:]\n",
    "        labels[i+10,:,:,:] = img[512:,:512,:]\n",
    "        labels[i+11,:,:,:] = img[512:,512:,:]     \n",
    "#      augmentation (left to right flip)         \n",
    "        labels[i+12,:,:,:] =   np.flip(img[:512,:512,:],1)\n",
    "        labels[i+13,:,:,:] = np.flip(img[:512,512:,:],1)\n",
    "        labels[i+14,:,:,:] = np.flip(img[512:,:512,:],1)\n",
    "        labels[i+15,:,:,:] = np.flip(img[512:,512:,:],1)  \n",
    "        i += 16        \n",
    "#  shuffle        \n",
    "    labels = labels[idx,:,:,:]   \n",
    "#  split off test data (400 image/label pairs)   \n",
    "    split = 16*num_files - 400\n",
    "    x_train = images[:split,:,:,:]\n",
    "    y_train = labels[:split,:,:,:]\n",
    "    x_test = images[split:,:,:,:]\n",
    "    y_test = labels[split:,:,:,:]\n",
    "#  save compressed    \n",
    "    np.savez_compressed(folder+'/images_trainx.npz',x=x_train,y=y_train)\n",
    "    np.savez_compressed(folder+'/images_testx.npz',x=x_test,y=y_test)\n",
    "\n",
    "make_traintest_arrays(train_folder)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSV5TcQzTaNM"
   },
   "source": [
    "### Train and test datasets\n",
    "\n",
    "The compressed .npz files have been uploaded  to the folder _Inria_ on my Google Drive and are public. Simply copy the folder to your own Drive.\n",
    " \n",
    "There are 2880 image/label pairs in all (the images are $512\\times 512\\times 3$ RGB cubes in unsigned byte format, the labels are $512\\times 512\\times 1$ in 32 bit float) in the compressed files. They can be kept in memory during training if enough RAM is available using the *tf.data.Dataset.from_tensor_slices()* function. Both the training and test data are batched (batch size 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5GO0gFvTSYD"
   },
   "outputs": [],
   "source": [
    "train_folder = '/content/drive/MyDrive/Inria'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaroz3HF1Rvs"
   },
   "outputs": [],
   "source": [
    "path = train_folder+'/images_trainx.npz'\n",
    "with np.load(path) as data:\n",
    "  train_examples = data['x']\n",
    "  train_labels = data['y']   \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels)).batch(4)\n",
    "\n",
    "path = train_folder+'/images_testx.npz'\n",
    "with np.load(path) as data:\n",
    "  test_examples = data['x']\n",
    "  test_labels = data['y']        \n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels)).batch(4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8krFQeonp2dL"
   },
   "source": [
    "Alternatively, if RAM is insufficient, the train/test data can be pipelined using the *tf.data.Dataset.from_generator()* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-El2oKT7j1m_"
   },
   "outputs": [],
   "source": [
    "class MakeDataset():\n",
    "\n",
    "    def _generator(path):\n",
    "        with np.load(path) as data:\n",
    "            examples = data['x'] \n",
    "            labels = data['y']\n",
    "            for index, example in enumerate(examples):\n",
    "                yield (example,labels[index])\n",
    "\n",
    "    def __new__(cls,path):\n",
    "        return tf.data.Dataset.from_generator( cls._generator, \n",
    "               output_signature = ( tf.TensorSpec(shape=(512,512,3),dtype=tf.uint8),\n",
    "               tf.TensorSpec(shape=(512,512,1),dtype=tf.float32) ),\n",
    "               args = (path,) ).batch(4)\n",
    "\n",
    "train_dataset = MakeDataset(train_folder+'/images_trainx.npz')\n",
    "test_dataset = MakeDataset(train_folder+'/images_testx.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLNeqYjb-Rgm"
   },
   "source": [
    "Here are some examples from a train batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqzI3XSdTaNO"
   },
   "outputs": [],
   "source": [
    "train_examples, train_labels = next(train_dataset.as_numpy_iterator())\n",
    "fig, ax = plt.subplots(2,4,figsize=(20,10))\n",
    "for i in range(4):\n",
    "    ax[0,i].imshow(train_examples[i])\n",
    "    ax[1,i].imshow(np.reshape(train_labels[i],(512,512)),cmap = plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_39aX7p51Rvy"
   },
   "source": [
    "### The UNet FCN\n",
    "We'll program here a \"vanilla\" UNet based on the architecture described in [de Jong et al. (2019)](https://export.arxiv.org/pdf/1812.05815v2). It consists of an encoder and decoder section with lateral connections between the two, which in the literature is usually displayed in the form of a \"U\". The encoder section is a series of 5 pairs of Conv2D convolutional layers, with successively doubling numbers of filters (64, 128, ...), connected by MaxPooling2D layers which successively halve the image dimensions (512, 256, ...). The decoder section reverses the process with the help of upsampling Conv2DTranspose layers, ultimately reconstructing the input image signal at the network output. The input to each upsampling layer consists of the output from the preceding layer, merged (concatenated) with the output of the corresponding Conv2d layer from the encoding section. The idea is to restore higher resolution details lost during the image compression (encoding) phase while decoding takes place. The model takes as input a three channel RGB image in _np.uint8_ format and outputs an 1-channel image in _np.float32_ format. The output of the last Conv2D layer is passed through a sigmoid activation function.\n",
    "\n",
    "Since the convolutional layers are all identical except for the number of filters, and since each is followed by a [batch normalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) layer, it improves readability to use some shortcut functions. These can be conveniently programmed with the tensorflow _sequential API_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qevr3savwBLb"
   },
   "outputs": [],
   "source": [
    "# shortcuts\n",
    "def conv2d(filters):\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(filters,3,padding=\"same\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu')])\n",
    "def conv2dtranspose(filters):\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2DTranspose(filters,3,strides=2,padding=\"same\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu')])\n",
    "def maxpooling2d():\n",
    "    return tf.keras.layers.MaxPooling2D(pool_size=2,strides=2,padding=\"same\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcToiWrgwFKI"
   },
   "source": [
    "Here is the full UNet model written in tensorflow's _functional API_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRfxnFYf1Rvy"
   },
   "outputs": [],
   "source": [
    "# UNet\n",
    "def unet_model(num_channels=3,image_size=512):    \n",
    "    inputs = tf.keras.layers.Input(shape=(image_size,image_size,num_channels))\n",
    "    rescaled= tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
    "#  coder     \n",
    "    conv11 = conv2d(64)(rescaled)\n",
    "    conv12 = conv2d(64)(conv11)\n",
    "    max_pool1 = maxpooling2d()(conv12)\n",
    "    conv21 = conv2d(128)(max_pool1)\n",
    "    conv22 = conv2d(128)(conv21)\n",
    "    max_pool2 = maxpooling2d()(conv22)\n",
    "    conv31 = conv2d(256)(max_pool2)\n",
    "    conv32 = conv2d(256)(conv31)\n",
    "    max_pool3 = maxpooling2d()(conv32)\n",
    "    conv41 = conv2d(512)(max_pool3)\n",
    "    conv42 = conv2d(512)(conv41)\n",
    "    max_pool4 = maxpooling2d()(conv42)\n",
    "    conv51 = conv2d(1024)(max_pool4)\n",
    "    conv52 = conv2d(1024)(conv51)\n",
    "#  decoder    \n",
    "    uconv51 = conv2dtranspose(512)(conv52)\n",
    "    merge_dec5 = tf.keras.layers.concatenate([conv42,uconv51],axis=3)\n",
    "    conv_dec_41 = conv2d(512)(merge_dec5)\n",
    "    conv_dec_42 = conv2d(512)(conv_dec_41)\n",
    "    uconv41 = conv2dtranspose(256)(conv_dec_42)\n",
    "    merge_dec4 = tf.keras.layers.concatenate([conv32,uconv41],axis=3)\n",
    "    conv_dec_31 = conv2d(256)(merge_dec4)\n",
    "    conv_dec_32 = conv2d(256)(conv_dec_31)\n",
    "    uconv31 = conv2dtranspose(128)(conv_dec_32)\n",
    "    merge_dec3 = tf.keras.layers.concatenate([conv22,uconv31],axis=3)\n",
    "    conv_dec_21 = conv2d(128)(merge_dec3)\n",
    "    conv_dec_22 = conv2d(128)(conv_dec_21)\n",
    "    uconv21 = conv2dtranspose(64)(conv_dec_22)\n",
    "    merge_dec2 = tf.keras.layers.concatenate([conv12,uconv21],axis=3)\n",
    "    conv_dec_11 = conv2d(64)(merge_dec2)\n",
    "    conv_dec_12 = conv2d(64)(conv_dec_11)\n",
    "#  output    \n",
    "    conv_dec_12 = conv2d(8)(conv_dec_12)\n",
    "    output = tf.keras.layers.Conv2D(1,1,activation = 'sigmoid')(conv_dec_12)\n",
    "    return tf.keras.Model(inputs = inputs, outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwqtWEKC1Rv0"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "model = unet_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vS1Oduor_7gH"
   },
   "source": [
    "We'll also use a standard Adam optimizer with a learning rate of 0.001 and a BinaryCrossentropy loss function corresponding to the sigmoid activation. The metric is _BinaryAccuracy()_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiGM7IB11Rv1"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1SDSxO61Rv2"
   },
   "source": [
    "### Training\n",
    "During training it is a good idea to include a callback which saves the model after each epoch whenever the training loss (or validation loss if overfitting might be an issue) has been reduced. The Colab notebook's (free) GPU runtime may be disconnected by the provider at any time during training, particularly when the number of epochs is large  and the model state will otherwise be lost. Since training data are scarce, we will use the test data for both validation and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pv-dqFGC1Rv2"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "callback = ModelCheckpoint(train_folder+'/unet_inria_modelxxx.h5', monitor='loss', save_best_only=True)\n",
    "history = model.fit(train_dataset, epochs=2, callbacks = [callback], validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYDtwKkf09to"
   },
   "source": [
    "Also the following hint is useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRVysAFvOpTc"
   },
   "outputs": [],
   "source": [
    "#Run this Javascript snippet in your browser console to prevent getting thrown out prematurely\n",
    "function KeepClicking(){\n",
    "console.log(\"Clicking\");\n",
    "document.querySelector(\"colab-connect-button\").click()\n",
    "}\n",
    "setInterval(KeepClicking,60000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGCyMBLqji7w"
   },
   "source": [
    "Reload the model if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsNDfEdJ5Xn0"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(train_folder+'/unet_inria_modelx.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_juRMqSDjoZN"
   },
   "source": [
    "This changes the learn rate if desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F52jYzdyS1EH"
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr.assign(0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb5R-Mokig6w"
   },
   "source": [
    "### Evaluation\n",
    "After training for 30 epochs at learn rate 0.001 and 10 epochs at learn rate of 0.0003 we get the following accuracy on the test/valodation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apx3iK7kilwS"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3dEkpYBNejJ"
   },
   "source": [
    "And, doing some visual testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ul4590rG1Rv3"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "testds = iter(test_dataset)\n",
    "test_example, test_label = next(testds)\n",
    "fig, ax = plt.subplots(1,3,figsize=(20,10))\n",
    "ax[0].imshow(test_example[i])\n",
    "label = np.reshape(test_label[i],(512,512))\n",
    "ax[1].imshow(label,cmap = plt.cm.gray)\n",
    "pred = model.predict(tf.reshape(test_example[i],(1,512,512,3)))\n",
    "pred = np.reshape(np.where(pred>0.4,1,0),(512,512))\n",
    "ax[2].imshow(pred,cmap = plt.cm.gray)\n",
    "ax[0].set_title('test image')\n",
    "ax[1].set_title('ground truth')\n",
    "ax[2].set_title('predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_5SxPuM_C54"
   },
   "source": [
    "The comparison metric used inthe INRIA benchmark competition was _intersection over union_ (IoU), referring to the sets of labeled and classified pixels. A value of one implies perfect reproduction of the label image, a value zero means no correpondance whatsoever. The cell below calculates the average IoU for the test image/label pairs (recall that they are stored in memory in batches of 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJYEAmN0atgU"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "sumIoU = 0\n",
    "for test_example, test_label in test_dataset:\n",
    "    for j in range(4):\n",
    "        label = np.reshape(test_label[j],(512,512))\n",
    "        example = np.reshape(test_example[j],(1,512,512,3))\n",
    "        pred = model.predict(example)\n",
    "        pred = np.reshape(np.where(pred>0.4,1,0),(512,512))\n",
    "        I = label*pred\n",
    "        U = label+pred-I\n",
    "        sU = np.sum(U)\n",
    "        if sU>0: # buildings in subscene?\n",
    "            sumIoU += np.sum(I)/sU\n",
    "            i += 1        \n",
    "sumIoU/i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VkP9bFcJkPT"
   },
   "source": [
    "This excellent result exceeds the best results obtained by [Maggiori et al. (2017)](https://hal.inria.fr/hal-01468452/document) in the initial benchmarking of the INRIA dataset using a multi-label perceptron (MLP) architecture (0.60). The [initial winners](https://hal.inria.fr/hal-01767807/document) of the competition, achieved IoUs of the order 0.7 with variations of the UNet model that we are using here, although [later submissions](https://project.inria.fr/aerialimagelabeling/leaderboard/) reported as much as 0.8. All of the competition exercises were performed at 30cm spatial resolution, whereas we are training at 1.2m resolution so that a direct comparison may be misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIgpKDmY_V1f"
   },
   "source": [
    "### New data (NAIP)\n",
    "Our next goal is to use the UNet model to identify buildings in the NAIP aerial imagery in the GEE archive, which is confined to the continental USA. We will therefore continue training the model with NAIP subsets which match the original training images over American cities, namely Austin (36 scenes) and Chicago (36 scenes). To this end the corresponding 72 label images were uploaded to GEE assets, and then the matching NAIP images (and the labels) were exported to Google Drive at a scale of 1m and with the same crs as the label images. The uploading had to be done image-for-image from the GEE asset menu. We can take advantage of the fact that the NAIP imagery is acquired repeatedly over several years, so that we can obtain repeated training examples measured at different times and with different sensors.\n",
    "\n",
    "Here is the export script for downloading the NAIP image/label pairs for 2012. This was repeated for 2015 1nd 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FW0GzngGXrP"
   },
   "outputs": [],
   "source": [
    "start = 1\n",
    "end = 36\n",
    "\n",
    "#images = ['projects/ee-mortcanty/assets/inria/austin'+str(i) for i in range(start,end+1)]\n",
    "#crs = 'EPSG:26914'\n",
    "#fnp = 'austin'\n",
    "\n",
    "images = ['projects/ee-mortcanty/assets/inria/chicago'+str(i) for i in range(start,end+1)]\n",
    "crs = 'EPSG:26916'\n",
    "fnp = 'chicago'\n",
    "\n",
    "for i in range(start-1,end):\n",
    "    filenameprefix = fnp+str(i+1)\n",
    "    lbl = ee.Image(images[i]) # the labels\n",
    "    naip = ee.ImageCollection('USDA/NAIP/DOQQ') \\\n",
    "                      .select(['R','G','B']) \\\n",
    "                      .filter(ee.Filter.date('2010-01-01', '2012-12-31')) \\\n",
    "                      .filterBounds(lbl.geometry()) \\\n",
    "                      .mosaic() \\\n",
    "                      .clip(lbl.geometry()) \n",
    "    gdexport = ee.batch.Export.image.toDrive(naip,\n",
    "                description='driveExportTask', \n",
    "                folder = 'naip_images_2012_chicago',\n",
    "                crs = crs,                             \n",
    "                fileNamePrefix=filenameprefix,scale=1,maxPixels=1e11)   \n",
    "    gdexport.start()\n",
    "    gdexport1 = ee.batch.Export.image.toDrive(lbl,\n",
    "                description='driveExportTask', \n",
    "                folder = 'naip_labels',\n",
    "                fileNamePrefix=filenameprefix,scale=1,maxPixels=1e11)   \n",
    "    gdexport1.start()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSzXOzkwN2Cl"
   },
   "source": [
    "Now the 1m resolution images and their labels are on Google Drive. We create the train/test datasets as before in compressed .npz format, including 2% linear stretch and histogram equalization augmentation as well with chaining of the 2015 and 2017 aquisition periods (leaving out the 2012 data as they may differ too much from the labels which were created in 2016). Since the size of the dataset is considerably smaller than the original INRIA images, this can be done in Colab directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsX_f87FL3zx"
   },
   "outputs": [],
   "source": [
    "def make_traintest_arrays_naip(folder):\n",
    "    from osgeo.gdalconst import GA_ReadOnly\n",
    "\n",
    "    gdal.AllRegister()\n",
    "    \n",
    "#  images\n",
    "    files = os.listdir(folder+'/naip_images_2015')\n",
    "    files.sort()\n",
    "    num_files = len(files)\n",
    "    idx = np.random.permutation(range(24*num_files)) \n",
    "    images = np.zeros((24*num_files,512,512,3),dtype=np.uint8)\n",
    "    i = 0\n",
    "    print('images array 2015:')\n",
    "    for file in files:\n",
    "        if i%10 == 0:\n",
    "            print( '%i '%i,end=' ')         \n",
    "        img = np.zeros((1024,1024,3),dtype=np.uint8)      \n",
    "        inDataset = gdal.Open(folder+'/naip_images_2015/'+file, GA_ReadOnly)\n",
    "        for b in range(3):\n",
    "            band = inDataset.GetRasterBand(b+1)\n",
    "            img[:,:,b] = band.ReadAsArray(0,0,1024,1024)        \n",
    "        images[i,:,:,:] =   img[:512,:512,:]\n",
    "        images[i+1,:,:,:] = img[:512,512:,:]\n",
    "        images[i+2,:,:,:] = img[512:,:512,:]\n",
    "        images[i+3,:,:,:] = img[512:,512:,:]   \n",
    "#      augmentation linear saturation stretch \n",
    "        img2pc = img*0\n",
    "        for k in range(3):\n",
    "            img2pc[:,:,k] = lin2pcstr(img[:,:,k])\n",
    "        images[i+4,:,:,:] = img2pc[:512,:512,:]\n",
    "        images[i+5,:,:,:] = img2pc[:512,512:,:]\n",
    "        images[i+6,:,:,:] = img2pc[512:,:512,:]\n",
    "        images[i+7,:,:,:] = img2pc[512:,512:,:]\n",
    "#      augmentation histogram equalization stretch        \n",
    "        imgheq = img*0\n",
    "        for k in range(3):\n",
    "            imgheq[:,:,k] = histeqstr(img[:,:,k])\n",
    "        images[i+8,:,:,:] = imgheq[:512,:512,:]\n",
    "        images[i+9,:,:,:] = imgheq[:512,512:,:]\n",
    "        images[i+10,:,:,:] = imgheq[512:,:512,:]\n",
    "        images[i+11,:,:,:] = imgheq[512:,512:,:] \n",
    "        i += 12       \n",
    "    print('\\nimages array 2017:')    \n",
    "    files = os.listdir(folder+'/naip_images_2017')\n",
    "    files.sort()\n",
    "    for file in files:\n",
    "        if i%10 == 0:\n",
    "            print( '%i '%i,end=' ')          \n",
    "        inDataset = gdal.Open(folder+'/naip_images_2017/'+file, GA_ReadOnly)\n",
    "        for b in range(3):\n",
    "            band = inDataset.GetRasterBand(b+1)\n",
    "            img[:,:,b] = band.ReadAsArray(0,0,1024,1024)\n",
    "        images[i,:,:,:] = img[:512,:512,:]\n",
    "        images[i+1,:,:,:] = img[:512,512:,:]\n",
    "        images[i+2,:,:,:] = img[512:,:512,:]\n",
    "        images[i+3,:,:,:] = img[512:,512:,:]\n",
    "#      augmentation linear saturation stretch \n",
    "        img2pc = img*0\n",
    "        for k in range(3):\n",
    "            img2pc[:,:,k] = lin2pcstr(img[:,:,k])\n",
    "        images[i+4,:,:,:] = img2pc[:512,:512,:]\n",
    "        images[i+5,:,:,:] = img2pc[:512,512:,:]\n",
    "        images[i+6,:,:,:] = img2pc[512:,:512,:]\n",
    "        images[i+7,:,:,:] = img2pc[512:,512:,:]\n",
    "#      augmentation histogram equalization stretch        \n",
    "        imgheq = img*0\n",
    "        for k in range(3):\n",
    "            imgheq[:,:,k] = histeqstr(img[:,:,k])\n",
    "        images[i+8,:,:,:] = imgheq[:512,:512,:]\n",
    "        images[i+9,:,:,:] = imgheq[:512,512:,:]\n",
    "        images[i+10,:,:,:] = imgheq[512:,:512,:]\n",
    "        images[i+11,:,:,:] = imgheq[512:,512:,:] \n",
    "        i += 12          \n",
    "#  shuffle        \n",
    "    images = images[idx,:,:,:]    \n",
    "    \n",
    "#  labels    \n",
    "    files = os.listdir(folder+'/naip_labels')\n",
    "    files.sort()\n",
    "    labels = np.zeros((12*num_files,512,512,1),dtype=np.float32)\n",
    "    i = 0\n",
    "    print('\\nlabels array:')\n",
    "    for file in files: \n",
    "        if i%10 == 0:\n",
    "            print( '%i '%i,end=' ') \n",
    "        inDataset = gdal.Open(folder+'/naip_labels/'+file, GA_ReadOnly)                              \n",
    "        band = inDataset.GetRasterBand(1)\n",
    "        img = np.reshape(band.ReadAsArray(0,0,1024,1024),(1024,1024,1))\n",
    "        img = np.where(img>200,1,0)       \n",
    "        labels[i,:,:,:] =   img[:512,:512,:]\n",
    "        labels[i+1,:,:,:] = img[:512,512:,:]\n",
    "        labels[i+2,:,:,:] = img[512:,:512,:]\n",
    "        labels[i+3,:,:,:] = img[512:,512:,:]\n",
    "        labels[i+4,:,:,:] =  img[:512,:512,:]\n",
    "        labels[i+5,:,:,:] = img[:512,512:,:]\n",
    "        labels[i+6,:,:,:] = img[512:,:512,:]\n",
    "        labels[i+7,:,:,:] = img[512:,512:,:]\n",
    "        labels[i+8,:,:,:] = img[:512,:512,:]\n",
    "        labels[i+9,:,:,:] = img[:512,512:,:]\n",
    "        labels[i+10,:,:,:] = img[512:,:512,:]\n",
    "        labels[i+11,:,:,:] = img[512:,512:,:]\n",
    "        i += 12\n",
    "    labels = np.concatenate((labels,labels))\n",
    "#  shuffle        \n",
    "    labels = labels[idx,:,:,:]   \n",
    "#  split off test data \n",
    "    split = 24*num_files - 400\n",
    "    x_train = images[:split,:,:,:]\n",
    "    y_train = labels[:split,:,:,:]\n",
    "    x_test = images[split:,:,:,:]\n",
    "    y_test = labels[split:,:,:,:]\n",
    "#  save compressed    \n",
    "    np.savez_compressed(folder+'/naip_images_trainx.npz',x_train=x_train,y_train=y_train)\n",
    "    np.savez_compressed(folder+'/naip_images_testx.npz',x_test=x_test,y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x21-68-ZL3zy"
   },
   "outputs": [],
   "source": [
    "make_traintest_arrays_naip(train_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gp5AO5qiODQx"
   },
   "source": [
    "The train/test data are then read into RAM: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1BP1-BdL3zz"
   },
   "outputs": [],
   "source": [
    "path = train_folder+'/naip_images_trainx.npz'\n",
    "with np.load(path) as data:\n",
    "  train_examples = data['x_train']\n",
    "  train_labels = data['y_train']   \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels)).batch(4)\n",
    "\n",
    "path = train_folder+'/naip_images_testx.npz'\n",
    "with np.load(path) as data:\n",
    "  test_examples = data['x_test']\n",
    "  test_labels = data['y_test']        \n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels)).batch(4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCe_3lGrO49g"
   },
   "source": [
    "Next we load inria-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jgvzCL9O9_V"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(train_folder+'/unet_inria_modelx.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eB6X0_sGag4F"
   },
   "source": [
    "And do some visual testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHmZkdBRgvgt"
   },
   "outputs": [],
   "source": [
    "# Visual\n",
    "i = 1\n",
    "testds = iter(test_dataset)\n",
    "#_, _ = next(testds)\n",
    "_, _ = next(testds)\n",
    "test_example, test_label = next(testds)\n",
    "fig, ax = plt.subplots(1,3,figsize=(20,10))\n",
    "ax[0].imshow(test_example[i])\n",
    "label = np.reshape(test_label[i],(512,512))\n",
    "ax[1].imshow(label,cmap = plt.cm.gray)\n",
    "pred = model.predict(tf.reshape(test_example[i],(1,512,512,3)))\n",
    "pred = np.reshape(np.where(pred>0.4,1,0),(512,512))\n",
    "ax[2].imshow(pred,cmap = plt.cm.gray)\n",
    "ax[0].set_title('test image')\n",
    "ax[1].set_title('ground truth')\n",
    "ax[2].set_title('predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQ7gsoSXiP6M"
   },
   "outputs": [],
   "source": [
    "# IoU\n",
    "i = 0\n",
    "sumIoU = 0\n",
    "for test_example, test_label in test_dataset:\n",
    "    for j in range(4):\n",
    "        label = np.reshape(test_label[j],(512,512))\n",
    "        example = np.reshape(test_example[j],(1,512,512,3))\n",
    "        pred = model.predict(example)\n",
    "        pred = np.reshape(np.where(pred>0.4,1,0),(512,512))\n",
    "        I = label*pred\n",
    "        U = label+pred-I\n",
    "        sU = np.sum(U)\n",
    "        if sU>0: # no buildings in subscene?\n",
    "            sumIoU += np.sum(I)/sU\n",
    "            i += 1        \n",
    "sumIoU/i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmJ18bNyPMSd"
   },
   "source": [
    "The IoU mtric is considerably worse than for the inria images, therefore we continue training, saving the model now as _unet_inria_modelxx.h5_:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfEP8wP7gcRV"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PhzyXCAPGOA"
   },
   "outputs": [],
   "source": [
    "callback = ModelCheckpoint(train_folder+'/unet_inria_modelxxx.h5', monitor='loss', save_best_only=True)\n",
    "history = model.fit(train_dataset, epochs=10, callbacks = [callback], validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmbCmVkKIuDh"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(train_folder+'/unet_inria_modelxx.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHHD2_Zug1nz"
   },
   "source": [
    "After training for about 20 epochs, we again do some visual testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0h0WYZOvCHe"
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "testds = iter(test_dataset)\n",
    "_, _ = next(testds)\n",
    "_, _ = next(testds)\n",
    "test_example, test_label = next(testds)\n",
    "fig, ax = plt.subplots(1,3,figsize=(20,10))\n",
    "ax[0].imshow(test_example[i])\n",
    "label = np.reshape(test_label[i],(512,512))\n",
    "ax[1].imshow(label,cmap = plt.cm.gray)\n",
    "pred = model.predict(tf.reshape(test_example[i],(1,512,512,3)))\n",
    "pred = np.reshape(np.where(pred>0.6,1,0),(512,512))\n",
    "ax[2].imshow(pred,cmap = plt.cm.gray)\n",
    "ax[0].set_title('test image')\n",
    "ax[1].set_title('ground truth')\n",
    "ax[2].set_title('predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNX_eVT7wX7R"
   },
   "source": [
    "It looks pretty good, but what about IoU for the NAIP test dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNvV9wTDwZTM"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "sumIoU = 0\n",
    "for test_example, test_label in test_dataset:\n",
    "    for j in range(4):\n",
    "        label = np.reshape(test_label[j],(512,512))\n",
    "        example = np.reshape(test_example[j],(1,512,512,3))\n",
    "        pred = model.predict(example)\n",
    "        pred = np.reshape(np.where(pred>0.4,1,0),(512,512))\n",
    "        I = label*pred\n",
    "        U = label+pred-I\n",
    "        sU = np.sum(U)\n",
    "        if sU>0: # no buildings in subscene?\n",
    "            sumIoU += np.sum(I)/sU\n",
    "            i += 1        \n",
    "sumIoU/i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSJfg4XXKsJX"
   },
   "source": [
    "Now we have regained the good result achieved by the INRIA competition winners! So lets try to classify some new data. The script _unetclassiy.py_ in the _scripts_ directory partitions an input image into $512\\times 512$ tiles starting at the upper left hand corner and passes each tile through the trained model to classify it. It loads the pre-trained model and uses the model's _predict()_ function to classify the tiles. Here is the help output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bQM_1ZbNSeV"
   },
   "outputs": [],
   "source": [
    "%run /content/drive/MyDrive/scripts/unetclassify -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NZBksMeNiVY"
   },
   "source": [
    "Here is a NAIP image exported from GEE from 2018 over the Lawrence Livermore Laboratories in California:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNaGOGjsNxSN"
   },
   "outputs": [],
   "source": [
    "%run /content/drive/MyDrive/scripts/dispms -f /content/drive/MyDrive/gee/livermore2018.tif -p [1,2,3] -e 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHJThy5tOl87"
   },
   "source": [
    "Classifying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bWeyqcAQDtf"
   },
   "outputs": [],
   "source": [
    "%run /content/drive/MyDrive/scripts/unetclassify  -m /content/drive/MyDrive/Inria/unet_inria_modelxx.h5 /content/drive/MyDrive/gee/livermore2018.tif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNYrzbHZVIjk"
   },
   "source": [
    "We predict the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgqx41ivQt4I"
   },
   "outputs": [],
   "source": [
    "%run /content/drive/MyDrive/scripts/dispms -f /content/drive/MyDrive/gee/livermore2018_cnn.tif  -d [0,0,4000,3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ql59uoUtPGC1"
   },
   "source": [
    "After downloading the above two images and then uploading to the EarthEngine, we can overlay them nicely (the images are shared on GEE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qu_1LR52QefP"
   },
   "outputs": [],
   "source": [
    "img = ee.Image('projects/ee-mortcanty/assets/livermore2018')\n",
    "lab = ee.Image('projects/ee-mortcanty/assets/livermore2018_cnn')\n",
    "\n",
    "location = ee.Geometry.Polygon(img.get('system:footprint').getInfo()['coordinates']).centroid().getInfo()['coordinates'][::-1]\n",
    "\n",
    "m = folium.Map(location=location, zoom_start=14, height=800, width=1000)\n",
    "\n",
    "m.add_ee_layer(img,{'min':0,'max':255},'NAIP')\n",
    "m.add_ee_layer(lab.updateMask(lab.gt(0)),{'min':0,'max':255,'palette':['black','red']},'Label')\n",
    "\n",
    "m.add_child(folium.LayerControl())\n",
    "Fullscreen().add_to(m)\n",
    "\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AB3TU6rT43h"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5ZWhIOKT2sU"
   },
   "source": [
    "With a very straghtforward implementation of the UNet architecture we have obtained a good symantic classifier for building recognition, one which is close to state-of-the-art for the small training datasets available. We have also seen that the classifier can be applied to new RGB imagery from other sensors."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Semantic Segmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
