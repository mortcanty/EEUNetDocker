{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFPmkPvl1Rvb"
   },
   "source": [
    "# Symantic Classification of Urban Settlements\n",
    "## Mort Canty\n",
    "## November, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0rGG5Zq-U_T"
   },
   "source": [
    "## Context\n",
    " \n",
    "[Fully convolutional neural networks](https://ieeexplore.ieee.org/document/7478072) (FCNs) are useful for object recognition and semantic image classification. In this notebook we examine a popular FCN architecture, called [UNet](https://arxiv.org/abs/1505.04597), for urban building recognition (houses, commercial edifices, etc.) in aerial or satellite imagery. [Here](https://arxiv.org/abs/2107.12283) for example is a very recent application of the UNet architecture for large scale semantic classification of building footprints over the entire African continent.  \n",
    "\n",
    "To train our model we will make use of the dataset  for the [INRIA Aerial Imaging Labeling Benchmark](https://hal.inria.fr/hal-01468452/document), published in 2016. The dataset, consisting of images and semantic labels, can be downloaded [here](https://project.inria.fr/aerialimagelabeling/), and a presentation of the most successful sematic classifiers determined in the early part of the benchmark competition is given [here](https://hal.inria.fr/hal-01767807/document). \n",
    "\n",
    "\n",
    " \n",
    "After programming and training a UNet network at a **fourfold reduced spatial resolution** to that used in the competition, we'll try to apply it images outside the INRIA training/test domain by classifying remote sensing images taken from the Google Earth Engine archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Tr3BY_mhv-1"
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-S9kNAbGO7e"
   },
   "outputs": [],
   "source": [
    "import ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25bIL8zWbZBp",
    "outputId": "35d48d2d-ae05-4694-d8d5-f410968e370e"
   },
   "outputs": [],
   "source": [
    "# Trigger the authentication flow.\n",
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XffQTYTtL14e"
   },
   "outputs": [],
   "source": [
    "# Initialize the library.\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M730JvEN1Rvm"
   },
   "outputs": [],
   "source": [
    "# Import python modules\n",
    "import numpy as np\n",
    "import scipy\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "from osgeo import gdal,gdalconst\n",
    "from osgeo.gdalconst import GA_ReadOnly\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBJjHr_AEXbQ"
   },
   "outputs": [],
   "source": [
    "# Import the Folium library.\n",
    "import folium\n",
    "from folium.plugins import Fullscreen\n",
    "\n",
    "# Define a method for displaying Earth Engine image tiles to folium map.\n",
    "def add_ee_layer(self, ee_image_object, vis_params, name):\n",
    "    map_id_dict = ee.Image(ee_image_object).getMapId(vis_params)\n",
    "    folium.raster_layers.TileLayer(\n",
    "        tiles = map_id_dict['tile_fetcher'].url_format,\n",
    "        attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
    "        name = name,\n",
    "        overlay = True,\n",
    "        control = True\n",
    "    ).add_to(self)\n",
    "# Add EE drawing method to folium.\n",
    "folium.Map.add_ee_layer = add_ee_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ojXn_BJ5Xnh",
    "outputId": "6550d98f-fd4d-4730-c2f0-8e2e2afe12fc"
   },
   "outputs": [],
   "source": [
    "# Colab only\n",
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "jjC6OEZkv585",
    "outputId": "589d41a6-5417-4b05-d610-5bd1188e1feb"
   },
   "outputs": [],
   "source": [
    "# query GPU device\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTGksZYmN2_c"
   },
   "source": [
    "## The training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e34hnigoL14l"
   },
   "source": [
    "To quote from the INRIA download site:\n",
    "\n",
    " _[the images] cover dissimilar urban settlements, ranging from densely populated areas (e.g., San Franciscoâ€™s financial district) to alpine towns (e.g,. Lienz in Austrian Tyrol)-_\n",
    "\n",
    "There are 180 image/label pairs in all. Here is is an example of a full $5000\\times 5000$-pixel image/label pair over Chicago after upload to GEE and projection onto a folium map. For comparison and for later use we also display the corresponding spatial subset of the GEE's [USDA NAIP dataset](https://developers.google.com/earth-engine/datasets/catalog/USDA_NAIP_DOQQ). Note the poorer spatial resolution of the NAIP image (circa 1m) relative to the INRIA image (30cm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "id": "dofmxlULF-0K",
    "outputId": "0fb77c58-a449-4e95-a552-80ef6e61d87b"
   },
   "outputs": [],
   "source": [
    "img = ee.Image('users/mortcanty/chicago3')\n",
    "label = ee.Image('users/mortcanty/chicago_label3')\n",
    "img_rgb = ee.Image.rgb(img.select(0),img.select(1),img.select(2))\n",
    "\n",
    "location = img.geometry().centroid().coordinates().getInfo()[::-1]\n",
    "m = folium.Map(location=location, zoom_start=15, height=800, width=1000)\n",
    "\n",
    "m.add_ee_layer(img_rgb, {'min': 0, 'max': 200}, 'INRIA Train Image')\n",
    "m.add_ee_layer(label, {'min': 0, 'max': 200}, 'INRIA Label Image')\n",
    "\n",
    "naip = ee.ImageCollection('USDA/NAIP/DOQQ') \\\n",
    "                      .select(['R','G','B']) \\\n",
    "                      .filter(ee.Filter.date('2015-01-01', '2015-12-31')) \\\n",
    "                      .filterBounds(img.geometry()) \\\n",
    "                      .mosaic() \\\n",
    "                      .clip(img.geometry()) \n",
    "\n",
    "naip_rgb = ee.Image.rgb(naip.select(0),naip.select(1),naip.select(2))\n",
    "m.add_ee_layer(naip, {'min': 0, 'max': 255}, 'NAIP Image')\n",
    "\n",
    "m.add_child(folium.LayerControl())\n",
    "\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJRZziWOhv-3"
   },
   "source": [
    "**Note: The following cells should be run on the user's local machine, especially if disk space is a problem on Google Drive. The INRIA training data occupy 13 GB before compression but only about 1.5 GB when compressed and augmented (see below). We'll assume that the train images are in the directory _train_folder/images_ and the labels in the diretory _train_folder/gt_.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uTLqltETaNK"
   },
   "outputs": [],
   "source": [
    "# local only (insert your path here)\n",
    "train_folder = '/media/mort/Crucial/imagery/Inria/AerialImageDataset/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HyjiIy81Rvn"
   },
   "source": [
    "### Numpy arrays for training and testing\n",
    "\n",
    "The following function (somewhat wastefully) clips the original $5000\\times 5000$ pixel images and labels to $4096\\times 4096$ and then compresses the result to $1024\\times 1024$, thus generating training and test image sets with approx 1.2m ground resolution. Each image (and its label) is then split into four $512\\times 512$ segments.  There are 720 image/label pairs, which are then augmented by  (1) by applying a linear 2% saturation stretch, (2) a histogram equalization and (3) horizontal flips. This is a so-called *regularization* measure which, it is hoped, will prevent overfitting and improve generalization. The image and label segments are shuffled randomly and the resulting numpy arrays are saved in compressed .npz format to the train_folder directory. This results in 2880 images altogether, of which 400 are split off for testing and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MyNU0ri1Rvo"
   },
   "outputs": [],
   "source": [
    "def bytestretch(arr,rng=None):\n",
    "#   byte stretch image numpy array\n",
    "    shp = arr.shape\n",
    "    arr = arr.ravel()\n",
    "    if rng is None:\n",
    "        rng = [np.min(arr),np.max(arr)]\n",
    "    tmp =  (arr-rng[0])*255.0/(rng[1]-rng[0])\n",
    "    tmp = np.where(tmp<0,0,tmp)  \n",
    "    tmp = np.where(tmp>255,255,tmp) \n",
    "    return np.asarray(np.reshape(tmp,shp),np.uint8)\n",
    "\n",
    "def histeqstr(x):\n",
    "#   histogram equalization stretch        \n",
    "    x = bytestretch(x)\n",
    "    hist,bin_edges = np.histogram(x,256,(0,256))\n",
    "    cdf = hist.cumsum()\n",
    "    lut = 255*cdf/float(cdf[-1])\n",
    "    return np.interp(x,bin_edges[:-1],lut)\n",
    "\n",
    "def lin2pcstr(x):\n",
    "#  2% linear stretch\n",
    "    x = bytestretch(x)\n",
    "    hist,bin_edges = np.histogram(x,256,(0,256))\n",
    "    cdf = hist.cumsum()\n",
    "    lower = 0\n",
    "    i = 0\n",
    "    while cdf[i] < 0.02*cdf[-1]:\n",
    "        lower += 1\n",
    "        i += 1\n",
    "    upper = 255\n",
    "    i = 255\n",
    "    while (cdf[i] > 0.98*cdf[-1]) and (upper>100):\n",
    "        upper -= 1\n",
    "        i -= 1\n",
    "    fp = (bin_edges-lower)*255/(upper-lower)\n",
    "    fp = np.where(bin_edges<=lower,0,fp)\n",
    "    fp = np.where(bin_edges>=upper,255,fp)\n",
    "    return np.interp(x,bin_edges,fp)          \n",
    "\n",
    "def make_traintest_arrays(folder,tmp='tmp.jpg'):\n",
    "\n",
    "    gdal.AllRegister()\n",
    "    translate_options = gdal.TranslateOptions(format='JPEG',outputType=gdalconst.GDT_Byte,\n",
    "                                              resampleAlg='bilinear',\n",
    "                                              width=1024,height=1024,srcWin=[0,0,4096,4096])\n",
    "#  images\n",
    "    files = os.listdir(folder+'/images')\n",
    "    files.sort()\n",
    "    num_files = len(files)\n",
    "    idx = np.random.permutation(range(16*num_files)) \n",
    "    images = np.zeros((16*num_files,512,512,3),dtype=np.uint8)\n",
    "    i = 0\n",
    "    print('images array:')\n",
    "    for file in files: \n",
    "        if i%10 == 0:\n",
    "            print( '%i '%i,end=' ') \n",
    "        gdal.Translate(tmp,folder+'/images/'+file,options=translate_options)\n",
    "        img = np.reshape(np.asarray(Image.open(tmp)),(1024,1024,3))\n",
    "        images[i,:,:,:] =   img[:512,:512,:]\n",
    "        images[i+1,:,:,:] = img[:512,512:,:]\n",
    "        images[i+2,:,:,:] = img[512:,:512,:]\n",
    "        images[i+3,:,:,:] = img[512:,512:,:]   \n",
    "#      augmentation linear saturation stretch \n",
    "        img2pc = img*0\n",
    "        for k in range(3):\n",
    "            img2pc[:,:,k] = lin2pcstr(img[:,:,k])\n",
    "        images[i+4,:,:,:] = img2pc[:512,:512,:]\n",
    "        images[i+5,:,:,:] = img2pc[:512,512:,:]\n",
    "        images[i+6,:,:,:] = img2pc[512:,:512,:]\n",
    "        images[i+7,:,:,:] = img2pc[512:,512:,:]\n",
    "#      augmentation histogram equalization stretch        \n",
    "        imgheq = img*0\n",
    "        for k in range(3):\n",
    "            imgheq[:,:,k] = histeqstr(img[:,:,k])\n",
    "        images[i+8,:,:,:] = imgheq[:512,:512,:]\n",
    "        images[i+9,:,:,:] = imgheq[:512,512:,:]\n",
    "        images[i+10,:,:,:] = imgheq[512:,:512,:]\n",
    "        images[i+11,:,:,:] = imgheq[512:,512:,:]        \n",
    "#      augmentation (left to right flip)         \n",
    "        images[i+12,:,:,:] = np.flip(img[:512,:512,:],1)\n",
    "        images[i+13,:,:,:] = np.flip(img[:512,512:,:],1)\n",
    "        images[i+14,:,:,:] = np.flip(img[512:,:512,:],1)\n",
    "        images[i+15,:,:,:] = np.flip(img[512:,512:,:],1)        \n",
    "        i += 16\n",
    "#  shuffle        \n",
    "    images = images[idx,:,:,:]    \n",
    "#  labels    \n",
    "    files = os.listdir(folder+'/gt')\n",
    "    files.sort()\n",
    "    labels = np.zeros((16*num_files,512,512,1),dtype=np.float32)\n",
    "    i = 0\n",
    "    print('\\nlabels array:')\n",
    "    for file in files:\n",
    "        if i%10 == 0:\n",
    "            print( '%i '%i,end=' ') \n",
    "        gdal.Translate(tmp,folder+'/gt/'+file,options=translate_options)\n",
    "        img = np.reshape(np.asarray(Image.open(tmp)),(1024,1024,1))\n",
    "        img = np.where(img>200,1,0)\n",
    "        labels[i,:,:,:] =   img[:512,:512,:]\n",
    "        labels[i+1,:,:,:] = img[:512,512:,:]\n",
    "        labels[i+2,:,:,:] = img[512:,:512,:]\n",
    "        labels[i+3,:,:,:] = img[512:,512:,:]\n",
    "#      augmentation        \n",
    "        labels[i+4,:,:,:] = img[:512,:512,:]\n",
    "        labels[i+5,:,:,:] = img[:512,512:,:]\n",
    "        labels[i+6,:,:,:] = img[512:,:512,:]\n",
    "        labels[i+7,:,:,:] = img[512:,512:,:]\n",
    "#      augmentation        \n",
    "        labels[i+8,:,:,:] = img[:512,:512,:]\n",
    "        labels[i+9,:,:,:] = img[:512,512:,:]\n",
    "        labels[i+10,:,:,:] = img[512:,:512,:]\n",
    "        labels[i+11,:,:,:] = img[512:,512:,:]     \n",
    "#      augmentation (left to right flip)         \n",
    "        labels[i+12,:,:,:] =   np.flip(img[:512,:512,:],1)\n",
    "        labels[i+13,:,:,:] = np.flip(img[:512,512:,:],1)\n",
    "        labels[i+14,:,:,:] = np.flip(img[512:,:512,:],1)\n",
    "        labels[i+15,:,:,:] = np.flip(img[512:,512:,:],1)  \n",
    "        i += 16        \n",
    "#  shuffle        \n",
    "    labels = labels[idx,:,:,:]   \n",
    "#  split off test data (400 image/label pairs)   \n",
    "    split = 16*num_files - 400\n",
    "    x_train = images[:split,:,:,:]\n",
    "    y_train = labels[:split,:,:,:]\n",
    "    x_test = images[split:,:,:,:]\n",
    "    y_test = labels[split:,:,:,:]\n",
    "#  save compressed    \n",
    "    np.savez_compressed(folder+'/images_trainx.npz',x_train=x_train,y_train=y_train)\n",
    "    np.savez_compressed(folder+'/images_testx.npz',x_test=x_test,y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngd1OdlmTaNL"
   },
   "outputs": [],
   "source": [
    "# local only\n",
    "make_traintest_arrays(train_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcnxzJ-ahv-8"
   },
   "source": [
    "**Note: The compressed .npz files should now be uploaded  to the folder _Inria_ on the user's Google Drive. What follows is run in Colab with a GPU runtime.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kN3j55x8TaNK"
   },
   "outputs": [],
   "source": [
    "# Colab only\n",
    "train_folder = '/content/drive/MyDrive/Inria'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IaMe3gsvtoS2",
    "outputId": "cb9fb1e0-ddc2-4d1d-8263-c15665435efa"
   },
   "outputs": [],
   "source": [
    "ls -l $train_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSV5TcQzTaNM"
   },
   "source": [
    "### Make train and test datasets in RAM\n",
    " \n",
    "There are 2880 image/label pairs in all (the images are $512\\times 512\\times 3$ RGB cubes in unsigned byte format, the labels are $512\\times 512\\times 1$ in 32 bit float). They can be kept in memory during training if enough RAM is available using the *tf.data.Dataset.from_tensor_slices()* function. Both the training and test data are batched (batch size 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaroz3HF1Rvs"
   },
   "outputs": [],
   "source": [
    "path = train_folder+'/images_trainx.npz'\n",
    "with np.load(path) as data:\n",
    "  train_examples = data['x_train']\n",
    "  train_labels = data['y_train']   \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels)).batch(4)\n",
    "\n",
    "path = train_folder+'/images_testx.npz'\n",
    "with np.load(path) as data:\n",
    "  test_examples = data['x_test']\n",
    "  test_labels = data['y_test']        \n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels)).batch(4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8krFQeonp2dL"
   },
   "source": [
    "### Make train and test datasets with generator pipeline\n",
    "Alternatively, if RAM is insufficient, the train/test data can be pipelined using the *tf.data.Dataset.from_generator()* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-El2oKT7j1m_"
   },
   "outputs": [],
   "source": [
    "class MakeDataset():\n",
    "\n",
    "    def _generator(path):\n",
    "        with np.load(path) as data:\n",
    "            train_examples = data['x_train'] \n",
    "            train_labels = data['y_train']\n",
    "            for index, example in enumerate(train_examples):\n",
    "                yield (example,train_labels[index])\n",
    "\n",
    "    def __new__(cls,path):\n",
    "        return tf.data.Dataset.from_generator( cls._generator, \n",
    "                                               output_signature = ( tf.TensorSpec(shape=(512,512,3),dtype=tf.uint8),\n",
    "                                                                   tf.TensorSpec(shape=(512,512,1),dtype=tf.float32) ),\n",
    "                                               args = (path,) \n",
    "                                             ).batch(4)\n",
    "\n",
    "train_dataset = MakeDataset(train_folder+'/images_trainx.npz')\n",
    "test_dataset = MakeDataset(train_folder+'/images_testx.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLNeqYjb-Rgm"
   },
   "source": [
    "Here are some examples from a train batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "vqzI3XSdTaNO",
    "outputId": "89a7c531-8237-43fa-cd04-ed9d79dcf24f"
   },
   "outputs": [],
   "source": [
    "train_examples, train_labels = next(train_dataset.as_numpy_iterator())\n",
    "fig, ax = plt.subplots(2,4,figsize=(20,10))\n",
    "for i in range(4):\n",
    "    ax[0,i].imshow(train_examples[i])\n",
    "    ax[1,i].imshow(np.reshape(train_labels[i],(512,512)),cmap = plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_39aX7p51Rvy"
   },
   "source": [
    "### The UNet FCN\n",
    "We'll program here a \"vanilla\" UNet based on the architecture described in [de Jong et al. (2019)](https://export.arxiv.org/pdf/1812.05815v2). It consists of an encoder and decoder section with lateral connections between the two, which in the literature is usually displayed in the form of a \"U\". The encoder section is a series of 5 pairs of Conv2D convolutional layers, with successively doubling numbers of filters (64, 128, ...), connected by MaxPooling2D layers which successively halve the image dimensions (512, 256, ...). The decoder section reverses the process with the help of upsampling Conv2DTranspose layers, ultimately reconstructing the input image signal at the network output. The input to each upsampling layer consists of the output from the preceding layer, merged (concatenated) with the output of the corresponding Conv2d layer from the encoding section. The idea is to restore higher resolution details lost during the image compression (encoding) phase while decoding takes place. The model takes as input a three channel RGB image in _np.uint8_ format and outputs an 1-channel image in _np.float32_ format. The output of the last Conv2D layer is passed through a sigmoid activation function.\n",
    "\n",
    "Since the convolutional layers are all identical except for the number of filters, and since each is followed by a [batch normalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) layer, it improves readability to use some shortcut functions. These can be conveniently programmed with the tensorflow _sequential API_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qevr3savwBLb"
   },
   "outputs": [],
   "source": [
    "# shortcuts\n",
    "def conv2d(filters):\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(filters,3,padding=\"same\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu')])\n",
    "def conv2dtranspose(filters):\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2DTranspose(filters,3,strides=2,padding=\"same\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu')])\n",
    "def maxpooling2d():\n",
    "    return tf.keras.layers.MaxPooling2D(pool_size=2,strides=2,padding=\"same\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcToiWrgwFKI"
   },
   "source": [
    "Here is the full UNet model written in tensorflow's _functional API_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRfxnFYf1Rvy"
   },
   "outputs": [],
   "source": [
    "# UNet\n",
    "def unet_model(num_channels=3,image_size=512):    \n",
    "    inputs = tf.keras.layers.Input(shape=(image_size,image_size,num_channels))\n",
    "    rescaled= tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
    "#  coder     \n",
    "    conv11 = conv2d(64)(rescaled)\n",
    "    conv12 = conv2d(64)(conv11)\n",
    "    max_pool1 = maxpooling2d()(conv12)\n",
    "    conv21 = conv2d(128)(max_pool1)\n",
    "    conv22 = conv2d(128)(conv21)\n",
    "    max_pool2 = maxpooling2d()(conv22)\n",
    "    conv31 = conv2d(256)(max_pool2)\n",
    "    conv32 = conv2d(256)(conv31)\n",
    "    max_pool3 = maxpooling2d()(conv32)\n",
    "    conv41 = conv2d(512)(max_pool3)\n",
    "    conv42 = conv2d(512)(conv41)\n",
    "    max_pool4 = maxpooling2d()(conv42)\n",
    "    conv51 = conv2d(1024)(max_pool4)\n",
    "    conv52 = conv2d(1024)(conv51)\n",
    "#  decoder    \n",
    "    uconv51 = conv2dtranspose(512)(conv52)\n",
    "    merge_dec5 = tf.keras.layers.concatenate([conv42,uconv51],axis=3)\n",
    "    conv_dec_41 = conv2d(512)(merge_dec5)\n",
    "    conv_dec_42 = conv2d(512)(conv_dec_41)\n",
    "    uconv41 = conv2dtranspose(256)(conv_dec_42)\n",
    "    merge_dec4 = tf.keras.layers.concatenate([conv32,uconv41],axis=3)\n",
    "    conv_dec_31 = conv2d(256)(merge_dec4)\n",
    "    conv_dec_32 = conv2d(256)(conv_dec_31)\n",
    "    uconv31 = conv2dtranspose(128)(conv_dec_32)\n",
    "    merge_dec3 = tf.keras.layers.concatenate([conv22,uconv31],axis=3)\n",
    "    conv_dec_21 = conv2d(128)(merge_dec3)\n",
    "    conv_dec_22 = conv2d(128)(conv_dec_21)\n",
    "    uconv21 = conv2dtranspose(64)(conv_dec_22)\n",
    "    merge_dec2 = tf.keras.layers.concatenate([conv12,uconv21],axis=3)\n",
    "    conv_dec_11 = conv2d(64)(merge_dec2)\n",
    "    conv_dec_12 = conv2d(64)(conv_dec_11)\n",
    "#  output    \n",
    "    conv_dec_12 = conv2d(8)(conv_dec_12)\n",
    "    output = tf.keras.layers.Conv2D(1,1,activation = 'sigmoid')(conv_dec_12)\n",
    "    return tf.keras.Model(inputs = inputs, outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwqtWEKC1Rv0",
    "outputId": "2c8cc0fb-9c00-4530-be0d-ab8b85286bc9"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "model = unet_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vS1Oduor_7gH"
   },
   "source": [
    "We'll also use a standard Adam optimizer with a learning rate of 0.001 and a BinaryCrossentropy loss function corresponding to the sigmoid activation. The metric is _BinaryAccuracy()_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiGM7IB11Rv1"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1SDSxO61Rv2"
   },
   "source": [
    "### Training\n",
    "During training it is a good idea to include a callback which saves the model after each epoch whenever the training loss (or validation loss if overfitting might be an issue) has been reduced. The Colab notebook's (free) GPU runtime may be disconnected by the provider at any time during training, particularly when the number of epochs is large (e.g., $\\approx 100$) and the model state will otherwise be lost. Since training data are scarce, we will use the test data for both validation and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pv-dqFGC1Rv2",
    "outputId": "a731ee92-d2e0-48f6-8d50-710314da7674"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "callback = ModelCheckpoint(train_folder+'/unet_inria_modelxxx.h5', monitor='loss', save_best_only=True)\n",
    "history = model.fit(train_dataset, epochs=2, callbacks = [callback], validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYDtwKkf09to"
   },
   "source": [
    "Also the following hint is useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRVysAFvOpTc"
   },
   "outputs": [],
   "source": [
    "#Run this Javascript snippet in your browser console to prevent getting thrown out prematurely\n",
    "function KeepClicking(){\n",
    "console.log(\"Clicking\");\n",
    "document.querySelector(\"colab-connect-button\").click()\n",
    "}\n",
    "setInterval(KeepClicking,60000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGCyMBLqji7w"
   },
   "source": [
    "Reload the model if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsNDfEdJ5Xn0"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(train_folder+'/unet_inria_modelx.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_juRMqSDjoZN"
   },
   "source": [
    "This changes the learn rate if desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F52jYzdyS1EH"
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr.assign(0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb5R-Mokig6w"
   },
   "source": [
    "### Evaluation\n",
    "After training for 30 epochs at learn rate 0.001 and 10 epochs at learn rate of 0.0003:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apx3iK7kilwS"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_5SxPuM_C54"
   },
   "source": [
    "### Intersection over Union (IoU) test\n",
    "\n",
    "The comparison metric used inthe INRIA benchmark competition was _intersection over union_ (IoU), referring to the sets of labeled and classified pixels. A value of one implies perfect reproduction of the label image, a value zero means no correpondance whatsoever. The cell below calculates the average IoU for the test image/label pairs (recall that they are stored in memory in batches of 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJYEAmN0atgU",
    "outputId": "ae2b419b-1a92-4d6c-ec2f-7440fb191744"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "sumIoU = 0\n",
    "for test_example, test_label in test_dataset:\n",
    "    for j in range(4):\n",
    "        label = np.reshape(test_label[j],(512,512))\n",
    "        example = np.reshape(test_example[j],(1,512,512,3))\n",
    "        pred = model.predict(example)\n",
    "        pred = np.reshape(np.where(pred>0.4,1,0),(512,512))\n",
    "        I = label*pred\n",
    "        U = label+pred-I\n",
    "        sU = np.sum(U)\n",
    "        if sU>0: # buildings in subscene?\n",
    "            sumIoU += np.sum(I)/sU\n",
    "            i += 1        \n",
    "sumIoU/i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VkP9bFcJkPT"
   },
   "source": [
    "This excellent result exceeds the best results obtained by [Maggiori et al. (2017)](https://hal.inria.fr/hal-01468452/document) in the initial benchmarking of the INRIA dataset using a multi-label perceptron (MLP) architecture (0.60). The [initial winners](https://hal.inria.fr/hal-01767807/document) of the competition, achieved IoUs of the order 0.7 with variations of the UNet model that we are using here, although later submissions reported as much as 0.8. All of the competition exercises were performed at 30cm spatial resolution, whereas we are training at 1.2m resolution so that a direct comparison may be misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMt10BDu1Rv3"
   },
   "source": [
    "### Visual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "Ul4590rG1Rv3",
    "outputId": "e71c793d-6938-434e-b906-874cf4c378c9"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "testds = iter(test_dataset)\n",
    "_, _ = next(testds)\n",
    "#_, _ = next(testds)\n",
    "test_example, test_label = next(testds)\n",
    "fig, ax = plt.subplots(1,3,figsize=(20,10))\n",
    "ax[0].imshow(test_example[i])\n",
    "label = np.reshape(test_label[i],(512,512))\n",
    "ax[1].imshow(label,cmap = plt.cm.gray)\n",
    "pred = model.predict(tf.reshape(test_example[i],(1,512,512,3)))\n",
    "pred = np.reshape(np.where(pred>0.4,1,0),(512,512))\n",
    "ax[2].imshow(pred,cmap = plt.cm.gray)\n",
    "ax[0].set_title('test image')\n",
    "ax[1].set_title('ground truth')\n",
    "ax[2].set_title('predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIgpKDmY_V1f"
   },
   "source": [
    "### New data (NAIP)\n",
    "Our next goal is to use the UNet model to identify buildings in the NAIP aerial imagery in the GEE arcihve, which is confined to the continental USA. We will therefore continue training the model with NAIP subsets which match the original training images over American cities, namely Austin (36 scenes) and Chicago (36 scenes). To this end we must first upload the corresponding 72 label images from our local machine to GEE assets, and then export the matching NAIP images (and the labels) to Google Drive at a scale of 1m and with the same crs as the label images. The uploading has, unfortunately, to be done image-for-image from the GEE asset menu. We can take advantage of the fact that the NAIP imagery is acquired repeatedly over several years, so that we can obtain repeated training examples measured at different times and with different sensors.\n",
    "\n",
    "Here is the export script for downloading the NAIP image/label pairs for 2012. We repeat this for 2015 1nd 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FW0GzngGXrP"
   },
   "outputs": [],
   "source": [
    "start = 1\n",
    "end = 36\n",
    "\n",
    "#images = ['projects/ee-mortcanty/assets/inria/austin'+str(i) for i in range(start,end+1)]\n",
    "#crs = 'EPSG:26914'\n",
    "#fnp = 'austin'\n",
    "\n",
    "images = ['projects/ee-mortcanty/assets/inria/chicago'+str(i) for i in range(start,end+1)]\n",
    "crs = 'EPSG:26916'\n",
    "fnp = 'chicago'\n",
    "\n",
    "for i in range(start-1,end):\n",
    "    filenameprefix = fnp+str(i+1)\n",
    "    lbl = ee.Image(images[i]) # the labels\n",
    "    naip = ee.ImageCollection('USDA/NAIP/DOQQ') \\\n",
    "                      .select(['R','G','B']) \\\n",
    "                      .filter(ee.Filter.date('2010-01-01', '2012-12-31')) \\\n",
    "                      .filterBounds(lbl.geometry()) \\\n",
    "                      .mosaic() \\\n",
    "                      .clip(lbl.geometry()) \n",
    "    gdexport = ee.batch.Export.image.toDrive(naip,\n",
    "                description='driveExportTask', \n",
    "                folder = 'naip_images_2012_chicago',\n",
    "                crs = crs,                             \n",
    "                fileNamePrefix=filenameprefix,scale=1,maxPixels=1e11)   \n",
    "    gdexport.start()\n",
    "    gdexport1 = ee.batch.Export.image.toDrive(lbl,\n",
    "                description='driveExportTask', \n",
    "                folder = 'naip_labels',\n",
    "                fileNamePrefix=filenameprefix,scale=1,maxPixels=1e11)   \n",
    "    gdexport1.start()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSzXOzkwN2Cl"
   },
   "source": [
    "Now the 1m resolution images and their labels are on Google Drive. After downloading to the local machine, we create the train/test datasets as before in compressed .npz format, including 2% linear stretch and histogram equalization augmentation as well with chaining of the 2015 and 2017 aquisition periods (leaving out the 2012 data as they may differ too much from the labels which were created in 2016):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsX_f87FL3zx"
   },
   "outputs": [],
   "source": [
    "def make_traintest_arrays_naip(folder):\n",
    "    from osgeo.gdalconst import GA_ReadOnly\n",
    "\n",
    "    gdal.AllRegister()\n",
    "    \n",
    "#  images\n",
    "    files = os.listdir(folder+'/naip_images_2015')\n",
    "    files.sort()\n",
    "    num_files = len(files)\n",
    "    idx = np.random.permutation(range(24*num_files)) \n",
    "    images = np.zeros((24*num_files,512,512,3),dtype=np.uint8)\n",
    "    i = 0\n",
    "    print('images array 2015:')\n",
    "    for file in files:\n",
    "        if i%10 == 0:\n",
    "            print( '%i '%i,end=' ')         \n",
    "        img = np.zeros((1024,1024,3),dtype=np.uint8)      \n",
    "        inDataset = gdal.Open(folder+'/naip_images_2015/'+file, GA_ReadOnly)\n",
    "        for b in range(3):\n",
    "            band = inDataset.GetRasterBand(b+1)\n",
    "            img[:,:,b] = band.ReadAsArray(0,0,1024,1024)        \n",
    "        images[i,:,:,:] =   img[:512,:512,:]\n",
    "        images[i+1,:,:,:] = img[:512,512:,:]\n",
    "        images[i+2,:,:,:] = img[512:,:512,:]\n",
    "        images[i+3,:,:,:] = img[512:,512:,:]   \n",
    "#      augmentation linear saturation stretch \n",
    "        img2pc = img*0\n",
    "        for k in range(3):\n",
    "            img2pc[:,:,k] = lin2pcstr(img[:,:,k])\n",
    "        images[i+4,:,:,:] = img2pc[:512,:512,:]\n",
    "        images[i+5,:,:,:] = img2pc[:512,512:,:]\n",
    "        images[i+6,:,:,:] = img2pc[512:,:512,:]\n",
    "        images[i+7,:,:,:] = img2pc[512:,512:,:]\n",
    "#      augmentation histogram equalization stretch        \n",
    "        imgheq = img*0\n",
    "        for k in range(3):\n",
    "            imgheq[:,:,k] = histeqstr(img[:,:,k])\n",
    "        images[i+8,:,:,:] = imgheq[:512,:512,:]\n",
    "        images[i+9,:,:,:] = imgheq[:512,512:,:]\n",
    "        images[i+10,:,:,:] = imgheq[512:,:512,:]\n",
    "        images[i+11,:,:,:] = imgheq[512:,512:,:] \n",
    "        i += 12       \n",
    "    print('\\nimages array 2017:')    \n",
    "    files = os.listdir(folder+'/naip_images_2017')\n",
    "    files.sort()\n",
    "    for file in files:\n",
    "        if i%10 == 0:\n",
    "            print( '%i '%i,end=' ')          \n",
    "        inDataset = gdal.Open(folder+'/naip_images_2017/'+file, GA_ReadOnly)\n",
    "        for b in range(3):\n",
    "            band = inDataset.GetRasterBand(b+1)\n",
    "            img[:,:,b] = band.ReadAsArray(0,0,1024,1024)\n",
    "        images[i,:,:,:] = img[:512,:512,:]\n",
    "        images[i+1,:,:,:] = img[:512,512:,:]\n",
    "        images[i+2,:,:,:] = img[512:,:512,:]\n",
    "        images[i+3,:,:,:] = img[512:,512:,:]\n",
    "#      augmentation linear saturation stretch \n",
    "        img2pc = img*0\n",
    "        for k in range(3):\n",
    "            img2pc[:,:,k] = lin2pcstr(img[:,:,k])\n",
    "        images[i+4,:,:,:] = img2pc[:512,:512,:]\n",
    "        images[i+5,:,:,:] = img2pc[:512,512:,:]\n",
    "        images[i+6,:,:,:] = img2pc[512:,:512,:]\n",
    "        images[i+7,:,:,:] = img2pc[512:,512:,:]\n",
    "#      augmentation histogram equalization stretch        \n",
    "        imgheq = img*0\n",
    "        for k in range(3):\n",
    "            imgheq[:,:,k] = histeqstr(img[:,:,k])\n",
    "        images[i+8,:,:,:] = imgheq[:512,:512,:]\n",
    "        images[i+9,:,:,:] = imgheq[:512,512:,:]\n",
    "        images[i+10,:,:,:] = imgheq[512:,:512,:]\n",
    "        images[i+11,:,:,:] = imgheq[512:,512:,:] \n",
    "        i += 12          \n",
    "#  shuffle        \n",
    "    images = images[idx,:,:,:]    \n",
    "    \n",
    "#  labels    \n",
    "    files = os.listdir(folder+'/naip_labels')\n",
    "    files.sort()\n",
    "    labels = np.zeros((12*num_files,512,512,1),dtype=np.float32)\n",
    "    i = 0\n",
    "    print('\\nlabels array:')\n",
    "    for file in files: \n",
    "        if i%10 == 0:\n",
    "            print( '%i '%i,end=' ') \n",
    "        inDataset = gdal.Open(folder+'/naip_labels/'+file, GA_ReadOnly)                              \n",
    "        band = inDataset.GetRasterBand(1)\n",
    "        img = np.reshape(band.ReadAsArray(0,0,1024,1024),(1024,1024,1))\n",
    "        img = np.where(img>200,1,0)       \n",
    "        labels[i,:,:,:] =   img[:512,:512,:]\n",
    "        labels[i+1,:,:,:] = img[:512,512:,:]\n",
    "        labels[i+2,:,:,:] = img[512:,:512,:]\n",
    "        labels[i+3,:,:,:] = img[512:,512:,:]\n",
    "        labels[i+4,:,:,:] =  img[:512,:512,:]\n",
    "        labels[i+5,:,:,:] = img[:512,512:,:]\n",
    "        labels[i+6,:,:,:] = img[512:,:512,:]\n",
    "        labels[i+7,:,:,:] = img[512:,512:,:]\n",
    "        labels[i+8,:,:,:] = img[:512,:512,:]\n",
    "        labels[i+9,:,:,:] = img[:512,512:,:]\n",
    "        labels[i+10,:,:,:] = img[512:,:512,:]\n",
    "        labels[i+11,:,:,:] = img[512:,512:,:]\n",
    "        i += 12\n",
    "    labels = np.concatenate((labels,labels))\n",
    "#  shuffle        \n",
    "    labels = labels[idx,:,:,:]   \n",
    "#  split off test data \n",
    "    split = 24*num_files - 400\n",
    "    x_train = images[:split,:,:,:]\n",
    "    y_train = labels[:split,:,:,:]\n",
    "    x_test = images[split:,:,:,:]\n",
    "    y_test = labels[split:,:,:,:]\n",
    "#  save compressed    \n",
    "    np.savez_compressed(folder+'/naip_images_trainx.npz',x_train=x_train,y_train=y_train)\n",
    "    np.savez_compressed(folder+'/naip_images_testx.npz',x_test=x_test,y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x21-68-ZL3zy",
    "outputId": "30b413d3-ed87-42a3-e8b9-7f2d5e7388d8"
   },
   "outputs": [],
   "source": [
    "make_traintest_arrays_naip(train_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gp5AO5qiODQx"
   },
   "source": [
    "Then the .npz files are again uploaded back to the Google Drive and the train/test data read into RAM: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1BP1-BdL3zz"
   },
   "outputs": [],
   "source": [
    "path = train_folder+'/naip_images_trainx.npz'\n",
    "with np.load(path) as data:\n",
    "  train_examples = data['x_train']\n",
    "  train_labels = data['y_train']   \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels)).batch(4)\n",
    "\n",
    "path = train_folder+'/naip_images_testx.npz'\n",
    "with np.load(path) as data:\n",
    "  test_examples = data['x_test']\n",
    "  test_labels = data['y_test']        \n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels)).batch(4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCe_3lGrO49g"
   },
   "source": [
    "Next we load inria-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jgvzCL9O9_V"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(train_folder+'/unet_inria_modelx.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eB6X0_sGag4F"
   },
   "source": [
    "And do some visual testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "bHmZkdBRgvgt",
    "outputId": "8536b87f-8370-4104-b9fd-1fd121ecef2a"
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "testds = iter(test_dataset)\n",
    "#_, _ = next(testds)\n",
    "_, _ = next(testds)\n",
    "test_example, test_label = next(testds)\n",
    "fig, ax = plt.subplots(1,3,figsize=(20,10))\n",
    "ax[0].imshow(test_example[i])\n",
    "label = np.reshape(test_label[i],(512,512))\n",
    "ax[1].imshow(label,cmap = plt.cm.gray)\n",
    "pred = model.predict(tf.reshape(test_example[i],(1,512,512,3)))\n",
    "pred = np.reshape(np.where(pred>0.4,1,0),(512,512))\n",
    "ax[2].imshow(pred,cmap = plt.cm.gray)\n",
    "ax[0].set_title('test image')\n",
    "ax[1].set_title('ground truth')\n",
    "ax[2].set_title('predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hNx8ONUat4F"
   },
   "source": [
    "Here is the IoU for the naip images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NQ7gsoSXiP6M",
    "outputId": "77700aaa-4846-4a09-ec0b-f9c9e240e6ca"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "sumIoU = 0\n",
    "for test_example, test_label in test_dataset:\n",
    "    for j in range(4):\n",
    "        label = np.reshape(test_label[j],(512,512))\n",
    "        example = np.reshape(test_example[j],(1,512,512,3))\n",
    "        pred = model.predict(example)\n",
    "        pred = np.reshape(np.where(pred>0.4,1,0),(512,512))\n",
    "        I = label*pred\n",
    "        U = label+pred-I\n",
    "        sU = np.sum(U)\n",
    "        if sU>0: # no buildings in subscene?\n",
    "            sumIoU += np.sum(I)/sU\n",
    "            i += 1        \n",
    "sumIoU/i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmJ18bNyPMSd"
   },
   "source": [
    "It is considerably worse than for the inria images, therefore we continue training, saving the model now as _unet_inria_modelxx.h5_:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfEP8wP7gcRV"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0003),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PhzyXCAPGOA",
    "outputId": "0b4a72f0-90ec-4811-f227-20577bcf8479"
   },
   "outputs": [],
   "source": [
    "callback = ModelCheckpoint(train_folder+'/unet_inria_modelxxx.h5', monitor='loss', save_best_only=True)\n",
    "history = model.fit(train_dataset, epochs=10, callbacks = [callback], validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmbCmVkKIuDh"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(train_folder+'/unet_inria_modelxx.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHHD2_Zug1nz"
   },
   "source": [
    "After training for about 20 epochs, we again do some visual testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "c0h0WYZOvCHe",
    "outputId": "67c7cfec-e5b4-488f-9d7a-68926878e028"
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "testds = iter(test_dataset)\n",
    "_, _ = next(testds)\n",
    "_, _ = next(testds)\n",
    "test_example, test_label = next(testds)\n",
    "fig, ax = plt.subplots(1,3,figsize=(20,10))\n",
    "ax[0].imshow(test_example[i])\n",
    "label = np.reshape(test_label[i],(512,512))\n",
    "ax[1].imshow(label,cmap = plt.cm.gray)\n",
    "pred = model.predict(tf.reshape(test_example[i],(1,512,512,3)))\n",
    "pred = np.reshape(np.where(pred>0.6,1,0),(512,512))\n",
    "ax[2].imshow(pred,cmap = plt.cm.gray)\n",
    "ax[0].set_title('test image')\n",
    "ax[1].set_title('ground truth')\n",
    "ax[2].set_title('predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNX_eVT7wX7R"
   },
   "source": [
    "It looks pretty good, but what about IoU for the NAIP test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNvV9wTDwZTM",
    "outputId": "8994c044-1a72-43b6-c300-5a5f9e6b3ec1"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "sumIoU = 0\n",
    "for test_example, test_label in test_dataset:\n",
    "    for j in range(4):\n",
    "        label = np.reshape(test_label[j],(512,512))\n",
    "        example = np.reshape(test_example[j],(1,512,512,3))\n",
    "        pred = model.predict(example)\n",
    "        pred = np.reshape(np.where(pred>0.4,1,0),(512,512))\n",
    "        I = label*pred\n",
    "        U = label+pred-I\n",
    "        sU = np.sum(U)\n",
    "        if sU>0: # no buildings in subscene?\n",
    "            sumIoU += np.sum(I)/sU\n",
    "            i += 1        \n",
    "sumIoU/i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSJfg4XXKsJX"
   },
   "source": [
    "Now we have regained the good result achieved by the INRIA competition winners! So lets try to classify some new data. The script _unetclassiy.py_ in the _scripts_ directory partitions an input image into $512\\times 512$ tiles starting at the upper left hand corner and passes each tile through the trained model to classify it. It loads the pre-trained model and uses the model's _predict()_ function to classify the tiles. Here is the help output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bQM_1ZbNSeV",
    "outputId": "96fc526a-4b45-4abd-84e7-001ba93fecbc"
   },
   "outputs": [],
   "source": [
    "%run /content/drive/MyDrive/scripts/unetclassify -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NZBksMeNiVY"
   },
   "source": [
    "Here is a NAIP image exported from GEE from (around) 2015 over the town of Springfield, Missouri:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "pNaGOGjsNxSN",
    "outputId": "52f6bf52-da04-44d2-cd0e-ab8b988d187e"
   },
   "outputs": [],
   "source": [
    "%run scripts/dispms -f /media/mort/Crucial/imagery/Inria/livermore2018.tif -p [1,2,3] -e 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHJThy5tOl87"
   },
   "source": [
    "Classifying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_bWeyqcAQDtf",
    "outputId": "8d0812ba-417c-41ed-a271-89cdebd0eacc"
   },
   "outputs": [],
   "source": [
    "%run scripts/unetclassify  -m /media/mort/Crucial/imagery/Inria/AerialImageDataset/train/unet_inria_modelxx.h5 /media/mort/Crucial/imagery/Inria/livermore2018.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNYrzbHZVIjk"
   },
   "source": [
    "We predict the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "id": "jgqx41ivQt4I",
    "outputId": "b08c0c3c-523c-49fd-e834-1e9fe19e6a30"
   },
   "outputs": [],
   "source": [
    "%run /content/drive/MyDrive/scripts/dispms -f /content/drive/MyDrive/gee/ogallala2018_cnn.tif  -d [2000,500,1000,1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ql59uoUtPGC1"
   },
   "source": [
    "After downloading the above two images and then uploading to the EarthEngine, we can overlay them nicely (the images are shared on GEE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "id": "qu_1LR52QefP",
    "outputId": "5a2e484c-7789-42ef-bd1e-4624636b01b0"
   },
   "outputs": [],
   "source": [
    "img = ee.Image('projects/ee-mortcanty/assets/ogallala2018')\n",
    "lab = ee.Image('projects/ee-mortcanty/assets/ogallala2018_cnn')\n",
    "\n",
    "location = ee.Geometry.Polygon(img.get('system:footprint').getInfo()['coordinates']).centroid().getInfo()['coordinates'][::-1]\n",
    "\n",
    "m = folium.Map(location=location, zoom_start=14, height=800, width=1000)\n",
    "\n",
    "m.add_ee_layer(img,{'min':0,'max':255},'NAIP')\n",
    "m.add_ee_layer(lab.updateMask(lab.gt(0)),{'min':0,'max':255,'palette':['black','red']},'Label')\n",
    "\n",
    "m.add_child(folium.LayerControl())\n",
    "Fullscreen().add_to(m)\n",
    "\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AB3TU6rT43h"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5ZWhIOKT2sU"
   },
   "source": [
    "With a very straghtforward implementation of the UNet architecture we have obtained a good symantic classifier for building recognition, one which is close to state-of-the-art for the small training datasets available. We have also seen that the classifier can be applied to new RGB imagery from other sensors."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "unet_inria_colabx.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
